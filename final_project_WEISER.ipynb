{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "import utils\n",
    "import multi_layer_perceptron as mlp\n",
    "import perceptron\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "SEED = 8 # after some testing this seed seems to work very well\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data and bring it into the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('emails.mat') #load matlab data using the loadmat function\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X data is given as a 57173x10000 dense matrix. For this project I want it to be row by row instead of column by column and to transpose the matrix it has to be decompressed. Decompression can be done by using the Objects method todense() which returns a standard numpy array. This array can then be transposed into the wanted format.\\\n",
    "The Y data is not compressed and can be read directly. It is however also stored as a column vector (1x10000) and for this project I want it as a row vector (10000) which can be archieved by reshaping it into just one dimension (-1 means the size of the dimension should be infered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(data['X'].astype('int16').todense().transpose(1,0)) # since this data is in a compressed sparse representation it is decompressed using the todense() method\n",
    "Y = np.asarray(data['Y'].reshape(-1)) # the labels are not compressed and can be read directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a one to all instances for the offset\n",
    "X = np.hstack([X,np.ones([X.shape[0], 1], dtype='int16')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a train/test split of 70/30 and did not create a validation split since there are no hyperparameters to be tuned with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, shuffle=True, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Perceptron Model and algorithm in NumPy. For documentation look at perceptron.py\n",
    "\n",
    "A pretrained version of this model is stored in models/perceptron_weights.npy. To look at the results, skip to the 'pre-trained' subsection after initiallizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'weigth_size': X_train.shape[1]}\n",
    "model = perceptron.Perceptron(config) # initiallization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.train_perceptron(model, (X_train, Y_train), (X_test, Y_test), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join('models', 'perceptron_weigths.npy'), 'wb') as f:\n",
    "#     np.save(f, model.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'perceptron_weigths.npy'), 'rb') as f:\n",
    "    model.weights = np.load(f)\n",
    "perceptron.test_perceptron(model, (X_train, Y_train), (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.empty(Y.shape[0])\n",
    "for i in range(len(Y)):\n",
    "    preds[i] = model.forward(X[i])\n",
    "preds = np.sign(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    preds, Y, name=\"Perceptron\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Perceptron prediction curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron as a more complex approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be trained in this notebook though I recommend running it directly from command line ($ python multi_layer_perceptron.py) especially when running this on a GPU since Jupyter notebooks will store values in your GPU's RAM even after the computation.\n",
    "\n",
    "A pre-trained model is again provided. To run it you can skip to the 'pre-trained' subsection after initiallization.\n",
    "\n",
    "Additional documentation is provided in multi_layer_perceptron.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading experiment configurations\n",
    "with open('config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "torch.manual_seed(config['manual_seed']) # set a manual seed for reproducability\n",
    "torch.cuda.manual_seed(config['manual_seed']) # also need to set it for runs on GPU\n",
    "\n",
    "# setting up the data in dataloaders\n",
    "data = scipy.io.loadmat('emails.mat')\n",
    "X_torch = np.asarray(data['X'].astype('float32').todense().transpose(1,0)) \n",
    "Y = np.asarray(data['Y'].astype('float32').reshape(-1)) \n",
    "Y_torch = (Y+1)/2 # labels are given as [-1, 1] but need to be [0,1] for binary Cross Entropy Loss\n",
    "config['in_size'] = X_torch.shape[1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_torch, Y_torch, train_size = config['train_size'], shuffle=True, random_state=config['manual_seed'])\n",
    "trainset = utils.Dataset(X_train, Y_train)\n",
    "train_loader = DataLoader(trainset, batch_size=config['batch_size'])\n",
    "testset = utils.Dataset(X_test, Y_test)\n",
    "test_loader = DataLoader(testset, batch_size=config['batch_size'])\n",
    "\n",
    "# initiallizing the model\n",
    "model = mlp.MLP(config) # initiallizes MLP model\n",
    "model = model.to(device) # only effects GPU environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train()\n",
    "optimizer = optim.Adam(params = model.parameters(), lr=config['lr']) # Adam seems to be a good multi-purpose Optimizer\n",
    "loss_fn = nn.BCELoss() # binary Cross Entropy Loss\n",
    "losses, train_accs, test_accs = mlp.train(model, optimizer, loss_fn, train_loader, config, test_loader) # trains the model and returns results\n",
    "\n",
    "# logging the training results\n",
    "log_data = {'losses': losses, 'train_accs': train_accs, 'test_accs': test_accs}\n",
    "with open(os.path.join('results', f'{config[\"manual_seed\"]}.json'), 'w') as f:\n",
    "    json.dump(log_data, f) # log training results\n",
    "print()\n",
    "\n",
    "# running tests and printing results\n",
    "model.eval()\n",
    "utils.run_analysis(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving ###\n",
    "# torch.save(model.state_dict(), os.path.join('models', 'mlp.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join('models', 'mlp.pth')))\n",
    "model.eval()\n",
    "utils.run_analysis(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(torch.tensor(X_torch, device=device))\n",
    "preds = preds.cpu().detach().numpy()\n",
    "preds = np.sign(preds-0.5)\n",
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    preds, Y, name=\"MLP\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('results', 'perceptron2.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=180)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(list(range(len(df))),df['losses'])\n",
    "# ax.legend(labels=['Churn'])\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=180)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(list(range(len(df))),df['train_accs'])\n",
    "ax.plot(list(range(len(df))),df['test_accs'])\n",
    "ax.legend(labels=['Training Data', 'Test Data'])\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('results', '42.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=180)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(list(range(len(df))),df['losses'])\n",
    "# ax.legend(labels=['Churn'])\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=180)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(list(range(len(df))),df['train_accs'])\n",
    "ax.plot(list(range(len(df))),df['test_accs'])\n",
    "ax.legend(labels=['Training Data', 'Test Data'])\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e48de6c1652fa5291f05a21cd90986cde0d8513b8171fa91986aa48d104e17c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
